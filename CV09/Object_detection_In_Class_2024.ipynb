{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5H74exrlMMa"
      },
      "source": [
        "# Th·ª±c h√†nh: Ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng YOLO\n",
        "Trong b√†i n√†y ta s·∫Ω th·ª±c h√†nh b√†i to√°n ph√°t hi·ªán qu·∫£ d·ª´a tr√™n bƒÉng chuy·ªÅn b·∫±ng m√¥ h√¨nh YOLO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeqnY5KjpBUb"
      },
      "source": [
        "# 1. C√†i ƒë·∫∑t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEXu7qOHlMMc"
      },
      "source": [
        "T·∫£i YOLO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PZstFSMfvJtE"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cloning into 'yolov3'...\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ultralytics/yolov3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTvV2DIwpi0T",
        "outputId": "a19acafd-8c20-426f-96c8-8690eae2bcd7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'yolov3'...\n",
            "remote: Enumerating objects: 10034, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 10034 (delta 4), reused 9 (delta 3), pack-reused 10017\u001b[K\n",
            "Receiving objects: 100% (10034/10034), 9.36 MiB | 14.32 MiB/s, done.\n",
            "Resolving deltas: 100% (6762/6762), done.\n"
          ]
        }
      ],
      "source": [
        "#############\n",
        "# 1 line of code here\n",
        "#############"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Volume in drive E is Learning\n",
            " Volume Serial Number is F4DE-EA33\n",
            "\n",
            " Directory of E:\\VinBigData\\CV_VIN\\datasets\n",
            "\n",
            "09/25/2024  11:16 AM    <DIR>          .\n",
            "09/25/2024  12:00 PM    <DIR>          ..\n",
            "09/25/2024  11:17 AM    <DIR>          datasets\n",
            "09/25/2024  11:15 AM    <DIR>          labels\n",
            "               0 File(s)              0 bytes\n",
            "               4 Dir(s)  54,510,804,992 bytes free\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lc-fub82pvdM",
        "outputId": "56b3705d-616f-40bd-b7f7-06ebbc406276"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "E:\\VinBigData\\CV_VIN\\datasets\\datasets\\yolov3\n"
          ]
        }
      ],
      "source": [
        "cd datasets/yolov3/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxOzD2kilMMe"
      },
      "source": [
        "C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt s·ª≠ d·ª•ng pip install requrement.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phIdfLIjp1qs",
        "outputId": "85f968c1-2503-4e7f-fd01-0d9850fd0b65"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 24.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "#############\n",
        "# 1 line of code here\n",
        "#############\n",
        "!pip install -qr requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOdE5IWylMMf"
      },
      "source": [
        "# 2. G√°n nh√£n d·ªØ li·ªáu  \n",
        "L√†m quen v·ªõi g√°n nh√£n d·ªØ li·ªáu b·∫±ng c√¥ng c·ª• LabelImg\n",
        "1. C√†i ƒë·∫∑t c√¥ng c·ª• LabelImg: https://github.com/tzutalin/labelImg  \n",
        "2. Ch·ªânh s·ª≠a predefined_classes.txt trong th∆∞ m·ª•c labelimg/data: Xo√° n·ªôi dung 15 class c√≥ s·∫µn v√† thay b·∫±ng coconut  \n",
        "3. M·ªü labelimg: python3 labelimg  \n",
        "4. Ch·ªçn th∆∞ m·ª•c samples, ch·ªçn ƒë·ªãnh d·∫°ng YOLO v√† th·ª±c hi·ªán ƒë√°nh nh√£n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BA75f7ImqNVK"
      },
      "source": [
        "# 3. Chu·∫©n b·ªã d·ªØ li·ªáu\n",
        "\n",
        "**M√¥ t·∫£**: Trong th∆∞ m·ª•c datasets ch·ª©a b·ªô d·ªØ li·ªáu s·ª≠ d·ª•ng trong b√†i th·ª±c h√†nh n√†y. Trong th∆∞ m·ª•c n√†y ch·ª©a th∆∞ m·ª•c con **images** g·ªìm to√†n b·ªô ·∫£nh d·ªØ li·ªáu; file **all_annotations.txt** ch·ª©a nh√£n bounding box c·ªßa to√†n b·ªô ·∫£nh (ƒë∆∞·ª£c ƒë√°nh nh√£n s·∫µn t·ª´ tr∆∞·ªõc). File **all_annotations.txt** g·ªìm nhi·ªÅu d√≤ng, m·ªói d√≤ng c√≥ format nh∆∞ sau:\n",
        "\n",
        "_image_path x1,y1,u1,v1,c1 x2,y2,u2,v2,c2 ... xn, yn, un, vn, cn_\n",
        "\n",
        "Trong ƒë√≥ (xi, yi) l√† t·ªça ƒë·ªô g√≥c tr√™n tr√°i c·ªßa ƒë·ªëi t∆∞·ª£ng th·ª© i, (ui, vi) l√† t·ªça ƒë·ªô g√≥c tr√™n ph·∫£i c·ªßa ƒë·ªëi t∆∞·ª£ng th·ª© i, ci l√† l·ªõp c·ªßa ƒë·ªëi t∆∞·ª£ng th·ª© i. Trong b√†i th·ª±c h√†nh n√†y ta ch·ªâ c√≥ m·ªôt l·ªõp d·ªØ li·ªáu duy nh·∫•t.\n",
        "\n",
        "\n",
        "**M√¥ t·∫£ format annotation m·ªõi**: M·ªói ·∫£nh ta s·∫Ω c√≥ 1 file .txt l∆∞u th√¥ng tin c√°c bounding box. File .txt n√†y c√≥ ƒë·ªãnh d·∫°ng nh∆∞ sau:\n",
        "\n",
        "*   M·ªói bounding box m·ªôt d√≤ng trong file\n",
        "*   Format c·ªßa t·ª´ng d√≤ng l√†: class x_center y_center width height\n",
        "*   C·∫ßn normalize x_center y_center width height v·ªÅ range [0, 1]\n",
        "*   class ƒë∆∞·ª£c ƒë√°nh s·ªë b·∫Øt ƒë·∫ßu t·ª´ 0\n",
        "\n",
        "**C√¥ng vi·ªác c·∫ßn th·ª±c hi·ªán**:  \n",
        "\n",
        "1.   T·∫°o th∆∞ m·ª•c datasets/labels ch·ª©a to√†n b·ªô c√°c file .txt (m·ªói ·∫£nh trong th∆∞ m·ª•c datasets/images ·ª©ng v·ªõi 1 file .txt trong th∆∞ m·ª•c datasets/labels) theo m√¥ t·∫£ ·ªü tr√™n)\n",
        "2.   Tham kh·∫£o file yolov3/data/coco.yaml, t·∫°o file yolov3/data/coconut.yaml ·ª©ng v·ªõi dataset ta v·ª´a x·ª≠ l√Ω\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQVrJdEQvlD0"
      },
      "source": [
        "Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkdgbs40yXGt",
        "outputId": "a3cbc31b-1c7a-4dd0-dc11-f7fb825b6fd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#############\n",
        "# your code here\n",
        "#############"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pPilJXQsbhS"
      },
      "source": [
        "Upload datasets.zip v√† unzip\n",
        "\n",
        "**L∆∞u √Ω:** Trong file zip s·∫µn ch·ªâ c√≥ 20 ·∫£nh. N·∫øu mu·ªën t·∫£i full d·ªØ li·ªáu, c√°c b·∫°n c√≥ th·ªÉ t·∫£i t·∫°i link sau: <a href=\"https://u.pcloud.link/publink/show?code=XZp1BhVZmloQdDfGGHFnNfoK7gX2vy66NlrX&fbclid=IwY2xjawFgZ1FleHRuA2FlbQIxMAABHTgHKueaW_1VSEELuOk6RSmPuJ57dld_msT0EKG2GVTOGaWah78rqHuXJg_aem_z34JWfq4mptE_3KBbeNKtw\">https://u.pcloud.link/publink/show?code=XZp1BhVZmloQdDfGGHFnNfoK7gX2vy66NlrX&fbclid=IwY2xjawFgZ1FleHRuA2FlbQIxMAABHTgHKueaW_1VSEELuOk6RSmPuJ57dld_msT0EKG2GVTOGaWah78rqHuXJg_aem_z34JWfq4mptE_3KBbeNKtw</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BowDZGxs1Nh",
        "outputId": "e73d2a78-887a-42d6-d1d9-fe2c77457360"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Volume in drive E is Learning\n",
            " Volume Serial Number is F4DE-EA33\n",
            "\n",
            " Directory of E:\\VinBigData\\CV_VIN\\datasets\\datasets\\yolov3\n",
            "\n",
            "09/25/2024  11:11 AM    <DIR>          .\n",
            "09/25/2024  11:17 AM    <DIR>          ..\n",
            "09/25/2024  11:11 AM             3,923 .dockerignore\n",
            "09/25/2024  11:11 AM    <DIR>          .github\n",
            "09/25/2024  11:11 AM             4,255 .gitignore\n",
            "09/25/2024  11:11 AM            14,346 benchmarks.py\n",
            "09/25/2024  11:11 AM               413 CITATION.cff\n",
            "09/25/2024  11:11 AM    <DIR>          classify\n",
            "09/25/2024  11:11 AM             5,082 CONTRIBUTING.md\n",
            "09/25/2024  11:11 AM    <DIR>          data\n",
            "09/25/2024  11:11 AM            23,521 detect.py\n",
            "09/25/2024  11:11 AM            71,108 export.py\n",
            "09/25/2024  11:11 AM            22,721 hubconf.py\n",
            "09/25/2024  11:11 AM            35,184 LICENSE\n",
            "09/25/2024  11:11 AM    <DIR>          models\n",
            "09/25/2024  11:11 AM             5,453 pyproject.toml\n",
            "09/25/2024  11:11 AM            42,860 README.md\n",
            "09/25/2024  11:11 AM            42,982 README.zh-CN.md\n",
            "09/25/2024  11:11 AM             1,638 requirements.txt\n",
            "09/25/2024  11:11 AM    <DIR>          segment\n",
            "09/25/2024  11:11 AM            41,113 train.py\n",
            "09/25/2024  11:11 AM            41,585 tutorial.ipynb\n",
            "09/25/2024  11:11 AM    <DIR>          utils\n",
            "09/25/2024  11:11 AM            31,207 val.py\n",
            "              16 File(s)        387,391 bytes\n",
            "               8 Dir(s)  54,510,804,992 bytes free\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EO3t-nYnnvHo",
        "outputId": "397254d2-3920-4f0d-82ff-31af6b771c32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "E:\\VinBigData\\CV_VIN\\datasets\n"
          ]
        }
      ],
      "source": [
        "cd E:\\VinBigData\\CV_VIN\\datasets\\"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmkNIBIDtKWr",
        "outputId": "8f33fcfa-a690-4a46-8df1-d715121adcbd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'unzip' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!unzip datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "WcfTEKGOqaf6"
      },
      "outputs": [],
      "source": [
        "# B∆∞·ªõc 1: Sinh file .txt ch·ª©a th√¥ng tin bounding box cho t·ª´ng ·∫£nh\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "IMAGE_FOLDER = 'datasets/images'\n",
        "ANNOTATION_FILE = 'datasets/all_annotations.txt'\n",
        "LABEL_FOLDER = 'datasets/labels'\n",
        "\n",
        "if not os.path.isdir(LABEL_FOLDER):\n",
        "  os.mkdir(LABEL_FOLDER)\n",
        "\n",
        "with open(ANNOTATION_FILE) as f:\n",
        "  for line in f:\n",
        "    data = line.split()\n",
        "\n",
        "    image_fp = data[0]\n",
        "    image_idx = image_fp.split('/')[-1].split('.')[0]\n",
        "    image = cv2.imread(image_fp)\n",
        "    if image is None:\n",
        "      continue\n",
        "    height, width = image.shape[:2]\n",
        "\n",
        "    normalized_bbox = []\n",
        "    for bbox in data[1:]:\n",
        "      x, y, u, v, cls = [int(a) for a in bbox.split(',')]\n",
        "      x_center = (x + u) / 2 / width\n",
        "      y_center = (y + v) / 2 / height\n",
        "      box_width = (u - x) / width\n",
        "      box_height =  (v - y) / height\n",
        "\n",
        "      normalized_bbox.append((cls, x_center, y_center, box_width, box_height))\n",
        "\n",
        "    with open(os.path.join(LABEL_FOLDER, image_idx + '.txt'), 'w') as g:\n",
        "      for bbox in normalized_bbox:\n",
        "        g.write(' '.join([str(s) for s in bbox]) + '\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0vHkVIpzZ5p",
        "outputId": "ea68528a-e256-4e8e-8080-810c8138fb24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "E:\\VinBigData\\CV_VIN\\datasets\\datasets\\yolov3\\data\n"
          ]
        }
      ],
      "source": [
        "cd datasets/yolov3/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "--nU0OWozj21"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'destination_path/coco.yaml'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[34], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshutil\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcoco.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdestination_path/coco.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Gia Bao\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\shutil.py:419\u001b[0m, in \u001b[0;36mcopy\u001b[1;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(dst):\n\u001b[0;32m    418\u001b[0m     dst \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dst, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(src))\n\u001b[1;32m--> 419\u001b[0m \u001b[43mcopyfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_symlinks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    420\u001b[0m copymode(src, dst, follow_symlinks\u001b[38;5;241m=\u001b[39mfollow_symlinks)\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dst\n",
            "File \u001b[1;32mc:\\Users\\Gia Bao\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\shutil.py:258\u001b[0m, in \u001b[0;36mcopyfile\u001b[1;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(src, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fsrc:\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 258\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fdst:\n\u001b[0;32m    259\u001b[0m             \u001b[38;5;66;03m# macOS\u001b[39;00m\n\u001b[0;32m    260\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m _HAS_FCOPYFILE:\n\u001b[0;32m    261\u001b[0m                 \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'destination_path/coco.yaml'"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "shutil.copy('coco.yaml', 'data/coco.yaml')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "lqRWiJHmOocg",
        "outputId": "0de827e6-5acb-400e-801c-be93f6e08868"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-083b56b11b6b>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    cd yolov3/data\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# B∆∞·ªõc 2: T·∫°o file coconut.yaml\n",
        "cd yolov3/data\n",
        "cp coco.yaml coconut.yaml\n",
        "\n",
        "# Ch·ªânh s·ª≠a yolov3/data/coconut.yaml v·ªõi n·ªôi dung nh∆∞ trong ph·∫ßn comment sau:\n",
        "\n",
        "'''\n",
        "path: ../datasets\n",
        "train: images/\n",
        "val: images/\n",
        "\n",
        "nc: 1\n",
        "\n",
        "names: ['coconut']\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cv4mHcBrPKJB"
      },
      "source": [
        "# 4. Hu·∫•n luy·ªán m√¥ h√¨nh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUBQcmqF19h3",
        "outputId": "39620429-3cfc-4fe9-97b7-61654f4ab482"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/yolov3\n"
          ]
        }
      ],
      "source": [
        "cd /content/yolov3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "al5OTHQDPhf3"
      },
      "source": [
        "## C√°ch 1: Finetune\n",
        "\n",
        "Ta s·∫Ω th·ª≠ fine-tune model v·ªõi pretrained c√≥ s·∫µn, c√°c tham s·ªë theo nh∆∞ m·∫∑c ƒë·ªãnh (tham kh·∫£o file yolov3/train.py), ƒë√¢y l√† c√°ch ch√∫ng ta lu√¥n th·ª≠ ƒë·∫ßu ti√™n v·ªõi m·ªôt b√†i to√°n m·ªõi. B·∫°n c√≥ th·ªÉ thay ƒë·ªïi s·ªë epoch ƒë·ªÉ r√∫t ng·∫Øn th·ªùi gian training. Sau khi k·∫øt th√∫c qu√° tr√¨nh training, h√£y nh√¨n v√†o ph·∫ßn log cu·ªëi c√πng ƒë·ªÉ x√°c ƒë·ªãnh v·ªã tr√≠ l∆∞u weights ƒë√£ train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ok3gZu7W0f2g",
        "outputId": "2d6f8a9d-838e-4965-8a5e-b9410868cb01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: (30 second timeout) \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B disabled due to login timeout.\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov3.pt, cfg=, data=data/coconut.yaml, hyp=data/hyps/hyp.scratch.yaml, epochs=20, batch_size=16, imgsz=416, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, adam=False, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, patience=100, freeze=0, save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov3 ‚úÖ\n",
            "YOLOv3 üöÄ v9.6.0-22-g0bbd055 torch 1.12.1+cu113 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv3 üöÄ runs (RECOMMENDED)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Downloading https://github.com/ultralytics/yolov3/releases/download/v9.6.0/yolov3.pt to yolov3.pt...\n",
            "100% 119M/119M [00:03<00:00, 38.7MB/s]\n",
            "\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1       928  models.common.Conv                      [3, 32, 3, 1]                 \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     20672  models.common.Bottleneck                [64, 64]                      \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    164608  models.common.Bottleneck                [128, 128]                    \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  8   2627584  models.common.Bottleneck                [256, 256]                    \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  8  10498048  models.common.Bottleneck                [512, 512]                    \n",
            "  9                -1  1   4720640  models.common.Conv                      [512, 1024, 3, 2]             \n",
            " 10                -1  4  20983808  models.common.Bottleneck                [1024, 1024]                  \n",
            " 11                -1  1   5245952  models.common.Bottleneck                [1024, 1024, False]           \n",
            " 12                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
            " 13                -1  1   4720640  models.common.Conv                      [512, 1024, 3, 1]             \n",
            " 14                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
            " 15                -1  1   4720640  models.common.Conv                      [512, 1024, 3, 1]             \n",
            " 16                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 17                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 18           [-1, 8]  1         0  models.common.Concat                    [1]                           \n",
            " 19                -1  1   1377792  models.common.Bottleneck                [768, 512, False]             \n",
            " 20                -1  1   1312256  models.common.Bottleneck                [512, 512, False]             \n",
            " 21                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 22                -1  1   1180672  models.common.Conv                      [256, 512, 3, 1]              \n",
            " 23                -2  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 24                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 25           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 26                -1  1    344832  models.common.Bottleneck                [384, 256, False]             \n",
            " 27                -1  2    656896  models.common.Bottleneck                [256, 256, False]             \n",
            " 28      [27, 22, 15]  1     32310  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [256, 512, 1024]]\n",
            "Model Summary: 333 layers, 61523734 parameters, 61523734 gradients, 155.3 GFLOPs\n",
            "\n",
            "Transferred 433/439 items from yolov3.pt\n",
            "Scaled weight_decay = 0.0005\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 72 weight, 75 weight (no decay), 75 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(always_apply=False, p=0.01, blur_limit=(3, 7)), MedianBlur(always_apply=False, p=0.01, blur_limit=(3, 7)), ToGray(always_apply=False, p=0.01), CLAHE(always_apply=False, p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '../datasets/labels' images and labels...20 found, 0 missing, 0 empty, 0 corrupted: 100% 20/20 [00:00<00:00, 324.80it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: ../datasets/labels.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '../datasets/labels.cache' images and labels... 20 found, 0 missing, 0 empty, 0 corrupted: 100% 20/20 [00:00<?, ?it/s]\n",
            "Plotting labels to runs/train/exp/labels.jpg... \n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m6.04 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ‚úÖ\n",
            "Image sizes 416 train, 416 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp\u001b[0m\n",
            "Starting training for 20 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      0/19     5.55G    0.1223    0.0619         0        61       416: 100% 2/2 [00:09<00:00,  4.91s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:01<00:00,  1.44s/it]\n",
            "                 all         20        104     0.0434      0.106     0.0165    0.00305\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      1/19     5.55G    0.1187   0.04432         0        28       416: 100% 2/2 [00:00<00:00,  2.47it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  2.48it/s]\n",
            "                 all         20        104     0.0448     0.0865     0.0163    0.00306\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      2/19     5.55G    0.1194   0.05066         0        34       416: 100% 2/2 [00:00<00:00,  2.58it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  3.16it/s]\n",
            "                 all         20        104     0.0511     0.0962     0.0184    0.00313\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      3/19     5.55G    0.1189   0.04794         0        34       416: 100% 2/2 [00:00<00:00,  2.98it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  2.77it/s]\n",
            "                 all         20        104      0.051      0.115     0.0186    0.00346\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      4/19     5.55G    0.1174   0.05679         0        48       416: 100% 2/2 [00:00<00:00,  3.08it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  2.54it/s]\n",
            "                 all         20        104     0.0535      0.115     0.0226    0.00422\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      5/19     5.55G     0.117    0.0559         0        41       416: 100% 2/2 [00:00<00:00,  2.71it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  3.31it/s]\n",
            "                 all         20        104     0.0565      0.106     0.0231    0.00463\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      6/19     5.55G    0.1155    0.0653         0        42       416: 100% 2/2 [00:00<00:00,  2.69it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  3.08it/s]\n",
            "                 all         20        104       0.06      0.115     0.0262    0.00509\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      7/19     5.55G    0.1123   0.06001         0        34       416: 100% 2/2 [00:00<00:00,  2.72it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  2.75it/s]\n",
            "                 all         20        104     0.0644      0.125     0.0304    0.00557\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      8/19     5.55G    0.1084   0.06294         0        34       416: 100% 2/2 [00:00<00:00,  2.73it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  2.99it/s]\n",
            "                 all         20        104      0.218     0.0673     0.0401    0.00673\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      9/19     5.55G    0.1085   0.06882         0        46       416: 100% 2/2 [00:00<00:00,  2.74it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  3.09it/s]\n",
            "                 all         20        104     0.0817      0.135     0.0419    0.00709\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     10/19     5.55G    0.1085   0.05616         0        37       416: 100% 2/2 [00:00<00:00,  2.76it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  2.78it/s]\n",
            "                 all         20        104      0.102      0.144     0.0524    0.00852\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     11/19     5.55G    0.1067   0.06275         0        33       416: 100% 2/2 [00:00<00:00,  2.98it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  2.87it/s]\n",
            "                 all         20        104      0.098      0.183     0.0548     0.0103\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     12/19     5.55G    0.1077   0.07933         0        48       416: 100% 2/2 [00:00<00:00,  2.79it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  3.62it/s]\n",
            "                 all         20        104      0.099      0.183     0.0586     0.0107\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     13/19     5.55G    0.1034   0.05839         0        22       416: 100% 2/2 [00:00<00:00,  3.29it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  2.88it/s]\n",
            "                 all         20        104      0.227      0.115      0.072     0.0135\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     14/19     5.55G    0.1026   0.07506         0        41       416: 100% 2/2 [00:00<00:00,  2.82it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  2.90it/s]\n",
            "                 all         20        104      0.219      0.125     0.0819     0.0164\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     15/19     5.55G    0.1016   0.07169         0        34       416: 100% 2/2 [00:00<00:00,  2.57it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  2.76it/s]\n",
            "                 all         20        104      0.342      0.115     0.0881     0.0153\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     16/19     5.55G    0.1007   0.06679         0        29       416: 100% 2/2 [00:00<00:00,  2.61it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  2.86it/s]\n",
            "                 all         20        104      0.282      0.125     0.0979     0.0194\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     17/19     5.55G     0.104   0.08023         0        53       416: 100% 2/2 [00:00<00:00,  2.82it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  2.39it/s]\n",
            "                 all         20        104      0.144      0.221      0.107     0.0214\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     18/19     5.55G   0.09873   0.07616         0        39       416: 100% 2/2 [00:00<00:00,  3.49it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  3.56it/s]\n",
            "                 all         20        104      0.218      0.192      0.112     0.0215\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     19/19     5.55G    0.0979   0.07868         0        38       416: 100% 2/2 [00:00<00:00,  2.66it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  2.86it/s]\n",
            "                 all         20        104      0.243      0.192      0.115     0.0209\n",
            "\n",
            "20 epochs completed in 0.032 hours.\n",
            "Optimizer stripped from runs/train/exp/weights/last.pt, 123.4MB\n",
            "Optimizer stripped from runs/train/exp/weights/best.pt, 123.4MB\n",
            "\n",
            "Validating runs/train/exp/weights/best.pt...\n",
            "Fusing layers... \n",
            "Model Summary: 261 layers, 61497430 parameters, 0 gradients, 154.5 GFLOPs\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  1.74it/s]\n",
            "                 all         20        104       0.22      0.192      0.112     0.0216\n",
            "Results saved to \u001b[1mruns/train/exp\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python train.py --img 416 --batch 16 --epochs 20 --data data/coconut.yaml --weights yolov3.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mG8oitE0n2LY"
      },
      "source": [
        "Th·ª≠ inference tr√™n 1 video s·ª≠ d·ª•ng model ƒë√£ finetune ·ªü tr√™n, ta s·∫Ω s·ª≠ d·ª•ng IoU threshold cho ph·∫ßn NMS l√† 0.5, threshold cho object score l√† 0.6  \n",
        "\n",
        "T·∫£i video t·∫°i link sau: ***https://u.pcloud.link/publink/show?code=XZziBhVZgRqC3dfFdWL1kB1zq0py6BbrMJny***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rvmwahle_vg-",
        "outputId": "01e847eb-d273-4d38-dca2-103c3688a350"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['runs/train/exp/weights/best.pt'], source=../datasets/coconut.mp4, imgsz=[416, 416], conf_thres=0.6, iou_thres=0.5, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
            "YOLOv3 üöÄ v9.6.0-3-gb870de5 torch 1.10.0+cu111 CUDA:0 (Tesla K80, 11441MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 261 layers, 61497430 parameters, 0 gradients, 154.7 GFLOPs\n",
            "Traceback (most recent call last):\n",
            "  File \"detect.py\", line 244, in <module>\n",
            "    main(opt)\n",
            "  File \"detect.py\", line 239, in main\n",
            "    run(**vars(opt))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\", line 28, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"detect.py\", line 95, in run\n",
            "    dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt and not jit)\n",
            "  File \"/content/yolov3/utils/datasets.py\", line 170, in __init__\n",
            "    raise Exception(f'ERROR: {p} does not exist')\n",
            "Exception: ERROR: /content/datasets/coconut.mp4 does not exist\n"
          ]
        }
      ],
      "source": [
        "!python detect.py --weights runs/train/exp/weights/best.pt --source ../datasets/coconut.mp4 --imgsz 416 --iou-thres 0.5 --conf-thres 0.6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nRsKhiTofTY"
      },
      "source": [
        "## C√°ch 2: T·ª± c·∫•u h√¨nh l·∫°i c√°c tham s·ªë\n",
        "\n",
        "ƒê·ªÉ l√†m ch·ªß vi·ªác training model, ta c√≥ th·ªÉ t·ª± thay ƒë·ªïi c√°c tham s·ªë ƒë·ªÉ ph√π h·ª£p v·ªõi t·ª´ng b·ªô d·ªØ li·ªáu kh√°c nhau. B√†i to√°n coconut detection kh√° ƒë∆°n gi·∫£n, v√¨ v·∫≠y c√°ch s·ª≠ d·ª•ng pretrained ·ªü tr√™n ƒë√£ cho k·∫øt qu·∫£ t·ªët, tuy nhi√™n v·ªõi nhi·ªÅu b√†i to√°n ph·ª©c t·∫°p h∆°n, ta s·∫Ω c·∫ßn thay ƒë·ªïi c√°c tham s·ªë sao cho ph√π h·ª£p. ·ªû ƒë√¢y ta s·∫Ω c·∫•u h√¨nh 2 ph·∫ßn ch√≠nh:\n",
        "\n",
        "\n",
        "*   Anchor boxes v√† Model architecture\n",
        "*   Model hyper parameters\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rr1anWZexdEE"
      },
      "source": [
        "### Anchor boxes v√† Model architecture\n",
        "\n",
        "- H√£y quan s√°t file yolov3/models/yolov3.yaml. Ta s·∫Ω t·∫°o file yolov3/models/coconut.yaml ƒë·ªÉ t·ª± c·∫•u h√¨nh.\n",
        "- Ph·∫ßn architecture g·ªìm backbone v√† head b·∫°n c√≥ th·ªÉ t·ª± thay ƒë·ªïi ·ªü nh√† ƒë·ªÉ th·ª≠ nghi·ªám, trong b√†i th·ª±c h√†nh n√†y ta s·∫Ω gi·ªØ nguy√™n architecture c·ªßa yolov3\n",
        "- Tham s·ªë _nc_ l√† s·ªë l∆∞·ª£ng l·ªõp c·ªßa b√†i to√°n, ta c·∫ßn thay ƒë·ªïi cho ph√π h·ª£p (·ªü c√°ch train 1 ·ªü tr√™n, ta kh√¥ng ch·ªâ ƒë·ªãnh file config yaml n√†y th√¨ model t·ª± hi·ªÉu l·∫•y tham s·ªë _nc_ trong file data/coconut.yaml)\n",
        "- Tham s·ªë anchors: Trong nhi·ªÅu b√†i to√°n ph·ª©c t·∫°p, vi·ªác t√¨m anchor boxes m·ªõi l√† c·∫ßn thi·∫øt. B·∫°n c√≥ th·ªÉ tham kh·∫£o repo sau: <a href=https://github.com/decanbay/YOLOv3-Calculate-Anchor-Boxes>https://github.com/decanbay/YOLOv3-Calculate-Anchor-Boxes </a>. H√£y th·ª≠ thay ƒë·ªïi anchor box c·ªßa m√¥ h√¨nh nh∆∞ sau:\n",
        "\n",
        "```\n",
        "P3/8: 77,34, 84,50, 87,70\n",
        "\n",
        "P4/16: 100,43, 107,59, 117,75\n",
        "\n",
        "P5/32: 128,50, 142,66, 153,87\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tztgjl7Lz7Sj"
      },
      "source": [
        "### Model hyper parameter\n",
        "\n",
        "C√°c si√™u tham s·ªë cho qu√° tr√¨nh hu·∫•n luy·ªán m√¥ h√¨nh ƒë∆∞·ª£c l∆∞u ·ªü c√°c file yaml trong th∆∞ m·ª•c yolov3/data/hyps. H√£y quan s√°t file hyp.scratch.yaml v√† thay ƒë·ªïi theo √Ω mu·ªën c·ªßa b·∫°n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAkEBi6V0Zm7"
      },
      "source": [
        "### Training model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAnNcXEmvnCq"
      },
      "outputs": [],
      "source": [
        "!python train.py --img 416 --batch 16 --epochs 20 --data data/coconut.yaml --weights yolov3.pt --cfg models/coconut.yaml --hyp data/hyps/hyp.scratch.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4cV_TtM02uL"
      },
      "source": [
        "# 5. L√†m ch·ªß pipeline inference model ƒë√£ train\n",
        "\n",
        "·ªû ph·∫ßn tr√™n ch√∫ng ta s·ª≠ d·ª•ng file detect.py ƒë·ªÉ infer m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán, ·ªü ph·∫ßn n√†y ch√∫ng ta s·∫Ω d·ª±a v√†o file detect.py, vi·∫øt 1 h√†m detector ng·∫Øn g·ªçn, nh·∫≠n v√†o 1 ·∫£nh, tr·∫£ l·∫°i t·ªça ƒë·ªô c√°c bounding box c·ªßa object. Vi·ªác vi·∫øt h√†m n√†y s·∫Ω gi√∫p ph·∫ßn l·∫≠p tr√¨nh thu·∫≠t to√°n tracking d·ªÖ d√†ng h∆°n trong c√°c bu·ªïi sau"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IjVeIWm1lmm",
        "outputId": "af84b978-4f6b-414f-db88-855851f424c5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "YOLOv3 üöÄ v9.6.0-3-gb870de5 torch 1.10.0+cu111 CPU\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 261 layers, 61497430 parameters, 0 gradients, 154.7 GFLOPs\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DetectMultiBackend() 32\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import cv2\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import numpy as np\n",
        "\n",
        "from models.common import DetectMultiBackend\n",
        "from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)\n",
        "from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox\n",
        "\n",
        "class YOLO():\n",
        "  def __init__(self):\n",
        "    self.img_size = [416, 416]\n",
        "    self.model_path = 'runs/train/exp/weights/best.pt'\n",
        "    self.iou_threshold = 0.5\n",
        "    self.conf_threshold = 0.6\n",
        "    self.device = 'cpu'\n",
        "    self.device = select_device(self.device)\n",
        "\n",
        "    # Initial model\n",
        "    self.model = self.load_models()\n",
        "    self.stride = self.model.stride\n",
        "    self.classes = self.model.names\n",
        "    print(self.model, self.stride)\n",
        "\n",
        "  def load_models(self):\n",
        "    model = DetectMultiBackend(self.model_path, device=self.device)\n",
        "    model.model.float()\n",
        "    # Warmup\n",
        "    model(torch.zeros(1, 3, *self.img_size).to(self.device).type_as(next(model.model.parameters())))\n",
        "    return model\n",
        "\n",
        "  def preprocess_image(self, image):\n",
        "    image = letterbox(image, self.img_size, stride=self.stride)[0]\n",
        "    image = image.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    image = np.ascontiguousarray(image)\n",
        "\n",
        "    return image\n",
        "\n",
        "  def detect(self, image):\n",
        "    im = np.array([self.preprocess_image(image)])\n",
        "    im = torch.from_numpy(im).to(self.device)\n",
        "    im = im.float() / 255.0\n",
        "    pred = self.model(im)\n",
        "    pred = non_max_suppression(pred, self.conf_threshold, self.iou_threshold)\n",
        "\n",
        "    original_image = image.copy()\n",
        "    centers = []\n",
        "    bboxes = []\n",
        "    obj_type = []\n",
        "\n",
        "    for i, det in enumerate(pred):\n",
        "      det[:, :4] = scale_coords(im.shape[2:], det[:, :4], image.shape).round()\n",
        "\n",
        "      for *xyxy, conf, cls in reversed(det):\n",
        "        xyxy = [float(a) for a in xyxy]\n",
        "        bboxes.append(np.array(xyxy))\n",
        "\n",
        "        x_center = (xyxy[0] + xyxy[2]) / 2\n",
        "        y_center = (xyxy[1] + xyxy[3]) / 2\n",
        "        centroid = np.array([[x_center], [y_center]])\n",
        "        centers.append(np.round(centroid))\n",
        "\n",
        "        predicted_class = self.classes[int(cls)]\n",
        "        obj_type.append(predicted_class)\n",
        "\n",
        "    return original_image, centers, bboxes, obj_type\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  image = cv2.imread('../datasets/images/5.png')\n",
        "  #############\n",
        "# Kh·ªüi t·∫°o YOLO\n",
        "#############\n",
        "  original_image, centers, bboxes, obj_type = detector.detect(image)\n",
        "  print(centers)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
