{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5H74exrlMMa"
      },
      "source": [
        "# Thực hành: Phát hiện đối tượng YOLO\n",
        "Trong bài này ta sẽ thực hành bài toán phát hiện quả dừa trên băng chuyền bằng mô hình YOLO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeqnY5KjpBUb"
      },
      "source": [
        "# 1. Cài đặt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEXu7qOHlMMc"
      },
      "source": [
        "Tải YOLO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PZstFSMfvJtE"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cloning into 'yolov3'...\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ultralytics/yolov3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTvV2DIwpi0T",
        "outputId": "a19acafd-8c20-426f-96c8-8690eae2bcd7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'yolov3'...\n",
            "remote: Enumerating objects: 10034, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 10034 (delta 4), reused 9 (delta 3), pack-reused 10017\u001b[K\n",
            "Receiving objects: 100% (10034/10034), 9.36 MiB | 14.32 MiB/s, done.\n",
            "Resolving deltas: 100% (6762/6762), done.\n"
          ]
        }
      ],
      "source": [
        "#############\n",
        "# 1 line of code here\n",
        "#############"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Volume in drive E is Learning\n",
            " Volume Serial Number is F4DE-EA33\n",
            "\n",
            " Directory of E:\\VinBigData\\CV_VIN\\datasets\n",
            "\n",
            "09/25/2024  11:16 AM    <DIR>          .\n",
            "09/25/2024  12:00 PM    <DIR>          ..\n",
            "09/25/2024  11:17 AM    <DIR>          datasets\n",
            "09/25/2024  11:15 AM    <DIR>          labels\n",
            "               0 File(s)              0 bytes\n",
            "               4 Dir(s)  54,510,804,992 bytes free\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lc-fub82pvdM",
        "outputId": "56b3705d-616f-40bd-b7f7-06ebbc406276"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "E:\\VinBigData\\CV_VIN\\datasets\\datasets\\yolov3\n"
          ]
        }
      ],
      "source": [
        "cd datasets/yolov3/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxOzD2kilMMe"
      },
      "source": [
        "Cài đặt các thư viện cần thiết sử dụng pip install requrement.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phIdfLIjp1qs",
        "outputId": "85f968c1-2503-4e7f-fd01-0d9850fd0b65"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 24.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "#############\n",
        "# 1 line of code here\n",
        "#############\n",
        "!pip install -qr requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOdE5IWylMMf"
      },
      "source": [
        "# 2. Gán nhãn dữ liệu  \n",
        "Làm quen với gán nhãn dữ liệu bằng công cụ LabelImg\n",
        "1. Cài đặt công cụ LabelImg: https://github.com/tzutalin/labelImg  \n",
        "2. Chỉnh sửa predefined_classes.txt trong thư mục labelimg/data: Xoá nội dung 15 class có sẵn và thay bằng coconut  \n",
        "3. Mở labelimg: python3 labelimg  \n",
        "4. Chọn thư mục samples, chọn định dạng YOLO và thực hiện đánh nhãn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BA75f7ImqNVK"
      },
      "source": [
        "# 3. Chuẩn bị dữ liệu\n",
        "\n",
        "**Mô tả**: Trong thư mục datasets chứa bộ dữ liệu sử dụng trong bài thực hành này. Trong thư mục này chứa thư mục con **images** gồm toàn bộ ảnh dữ liệu; file **all_annotations.txt** chứa nhãn bounding box của toàn bộ ảnh (được đánh nhãn sẵn từ trước). File **all_annotations.txt** gồm nhiều dòng, mỗi dòng có format như sau:\n",
        "\n",
        "_image_path x1,y1,u1,v1,c1 x2,y2,u2,v2,c2 ... xn, yn, un, vn, cn_\n",
        "\n",
        "Trong đó (xi, yi) là tọa độ góc trên trái của đối tượng thứ i, (ui, vi) là tọa độ góc trên phải của đối tượng thứ i, ci là lớp của đối tượng thứ i. Trong bài thực hành này ta chỉ có một lớp dữ liệu duy nhất.\n",
        "\n",
        "\n",
        "**Mô tả format annotation mới**: Mỗi ảnh ta sẽ có 1 file .txt lưu thông tin các bounding box. File .txt này có định dạng như sau:\n",
        "\n",
        "*   Mỗi bounding box một dòng trong file\n",
        "*   Format của từng dòng là: class x_center y_center width height\n",
        "*   Cần normalize x_center y_center width height về range [0, 1]\n",
        "*   class được đánh số bắt đầu từ 0\n",
        "\n",
        "**Công việc cần thực hiện**:  \n",
        "\n",
        "1.   Tạo thư mục datasets/labels chứa toàn bộ các file .txt (mỗi ảnh trong thư mục datasets/images ứng với 1 file .txt trong thư mục datasets/labels) theo mô tả ở trên)\n",
        "2.   Tham khảo file yolov3/data/coco.yaml, tạo file yolov3/data/coconut.yaml ứng với dataset ta vừa xử lý\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQVrJdEQvlD0"
      },
      "source": [
        "Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkdgbs40yXGt",
        "outputId": "a3cbc31b-1c7a-4dd0-dc11-f7fb825b6fd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#############\n",
        "# your code here\n",
        "#############"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pPilJXQsbhS"
      },
      "source": [
        "Upload datasets.zip và unzip\n",
        "\n",
        "**Lưu ý:** Trong file zip sẵn chỉ có 20 ảnh. Nếu muốn tải full dữ liệu, các bạn có thể tải tại link sau: <a href=\"https://u.pcloud.link/publink/show?code=XZp1BhVZmloQdDfGGHFnNfoK7gX2vy66NlrX&fbclid=IwY2xjawFgZ1FleHRuA2FlbQIxMAABHTgHKueaW_1VSEELuOk6RSmPuJ57dld_msT0EKG2GVTOGaWah78rqHuXJg_aem_z34JWfq4mptE_3KBbeNKtw\">https://u.pcloud.link/publink/show?code=XZp1BhVZmloQdDfGGHFnNfoK7gX2vy66NlrX&fbclid=IwY2xjawFgZ1FleHRuA2FlbQIxMAABHTgHKueaW_1VSEELuOk6RSmPuJ57dld_msT0EKG2GVTOGaWah78rqHuXJg_aem_z34JWfq4mptE_3KBbeNKtw</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BowDZGxs1Nh",
        "outputId": "e73d2a78-887a-42d6-d1d9-fe2c77457360"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Volume in drive E is Learning\n",
            " Volume Serial Number is F4DE-EA33\n",
            "\n",
            " Directory of E:\\VinBigData\\CV_VIN\\datasets\\datasets\\yolov3\n",
            "\n",
            "09/25/2024  11:11 AM    <DIR>          .\n",
            "09/25/2024  11:17 AM    <DIR>          ..\n",
            "09/25/2024  11:11 AM             3,923 .dockerignore\n",
            "09/25/2024  11:11 AM    <DIR>          .github\n",
            "09/25/2024  11:11 AM             4,255 .gitignore\n",
            "09/25/2024  11:11 AM            14,346 benchmarks.py\n",
            "09/25/2024  11:11 AM               413 CITATION.cff\n",
            "09/25/2024  11:11 AM    <DIR>          classify\n",
            "09/25/2024  11:11 AM             5,082 CONTRIBUTING.md\n",
            "09/25/2024  11:11 AM    <DIR>          data\n",
            "09/25/2024  11:11 AM            23,521 detect.py\n",
            "09/25/2024  11:11 AM            71,108 export.py\n",
            "09/25/2024  11:11 AM            22,721 hubconf.py\n",
            "09/25/2024  11:11 AM            35,184 LICENSE\n",
            "09/25/2024  11:11 AM    <DIR>          models\n",
            "09/25/2024  11:11 AM             5,453 pyproject.toml\n",
            "09/25/2024  11:11 AM            42,860 README.md\n",
            "09/25/2024  11:11 AM            42,982 README.zh-CN.md\n",
            "09/25/2024  11:11 AM             1,638 requirements.txt\n",
            "09/25/2024  11:11 AM    <DIR>          segment\n",
            "09/25/2024  11:11 AM            41,113 train.py\n",
            "09/25/2024  11:11 AM            41,585 tutorial.ipynb\n",
            "09/25/2024  11:11 AM    <DIR>          utils\n",
            "09/25/2024  11:11 AM            31,207 val.py\n",
            "              16 File(s)        387,391 bytes\n",
            "               8 Dir(s)  54,510,804,992 bytes free\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EO3t-nYnnvHo",
        "outputId": "397254d2-3920-4f0d-82ff-31af6b771c32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "E:\\VinBigData\\CV_VIN\\datasets\n"
          ]
        }
      ],
      "source": [
        "cd E:\\VinBigData\\CV_VIN\\datasets\\"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmkNIBIDtKWr",
        "outputId": "8f33fcfa-a690-4a46-8df1-d715121adcbd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'unzip' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!unzip datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "WcfTEKGOqaf6"
      },
      "outputs": [],
      "source": [
        "# Bước 1: Sinh file .txt chứa thông tin bounding box cho từng ảnh\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "IMAGE_FOLDER = 'datasets/images'\n",
        "ANNOTATION_FILE = 'datasets/all_annotations.txt'\n",
        "LABEL_FOLDER = 'datasets/labels'\n",
        "\n",
        "if not os.path.isdir(LABEL_FOLDER):\n",
        "  os.mkdir(LABEL_FOLDER)\n",
        "\n",
        "with open(ANNOTATION_FILE) as f:\n",
        "  for line in f:\n",
        "    data = line.split()\n",
        "\n",
        "    image_fp = data[0]\n",
        "    image_idx = image_fp.split('/')[-1].split('.')[0]\n",
        "    image = cv2.imread(image_fp)\n",
        "    if image is None:\n",
        "      continue\n",
        "    height, width = image.shape[:2]\n",
        "\n",
        "    normalized_bbox = []\n",
        "    for bbox in data[1:]:\n",
        "      x, y, u, v, cls = [int(a) for a in bbox.split(',')]\n",
        "      x_center = (x + u) / 2 / width\n",
        "      y_center = (y + v) / 2 / height\n",
        "      box_width = (u - x) / width\n",
        "      box_height =  (v - y) / height\n",
        "\n",
        "      normalized_bbox.append((cls, x_center, y_center, box_width, box_height))\n",
        "\n",
        "    with open(os.path.join(LABEL_FOLDER, image_idx + '.txt'), 'w') as g:\n",
        "      for bbox in normalized_bbox:\n",
        "        g.write(' '.join([str(s) for s in bbox]) + '\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0vHkVIpzZ5p",
        "outputId": "ea68528a-e256-4e8e-8080-810c8138fb24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "E:\\VinBigData\\CV_VIN\\datasets\\datasets\\yolov3\\data\n"
          ]
        }
      ],
      "source": [
        "cd datasets/yolov3/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "--nU0OWozj21"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'destination_path/coco.yaml'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[34], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshutil\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcoco.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdestination_path/coco.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Gia Bao\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\shutil.py:419\u001b[0m, in \u001b[0;36mcopy\u001b[1;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(dst):\n\u001b[0;32m    418\u001b[0m     dst \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dst, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(src))\n\u001b[1;32m--> 419\u001b[0m \u001b[43mcopyfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_symlinks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    420\u001b[0m copymode(src, dst, follow_symlinks\u001b[38;5;241m=\u001b[39mfollow_symlinks)\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dst\n",
            "File \u001b[1;32mc:\\Users\\Gia Bao\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\shutil.py:258\u001b[0m, in \u001b[0;36mcopyfile\u001b[1;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(src, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fsrc:\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 258\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fdst:\n\u001b[0;32m    259\u001b[0m             \u001b[38;5;66;03m# macOS\u001b[39;00m\n\u001b[0;32m    260\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m _HAS_FCOPYFILE:\n\u001b[0;32m    261\u001b[0m                 \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'destination_path/coco.yaml'"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "shutil.copy('coco.yaml', 'data/coco.yaml')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "lqRWiJHmOocg",
        "outputId": "0de827e6-5acb-400e-801c-be93f6e08868"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-083b56b11b6b>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    cd yolov3/data\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# Bước 2: Tạo file coconut.yaml\n",
        "cd yolov3/data\n",
        "cp coco.yaml coconut.yaml\n",
        "\n",
        "# Chỉnh sửa yolov3/data/coconut.yaml với nội dung như trong phần comment sau:\n",
        "\n",
        "'''\n",
        "path: ../datasets\n",
        "train: images/\n",
        "val: images/\n",
        "\n",
        "nc: 1\n",
        "\n",
        "names: ['coconut']\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cv4mHcBrPKJB"
      },
      "source": [
        "# 4. Huấn luyện mô hình"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUBQcmqF19h3",
        "outputId": "39620429-3cfc-4fe9-97b7-61654f4ab482"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/yolov3\n"
          ]
        }
      ],
      "source": [
        "cd /content/yolov3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "al5OTHQDPhf3"
      },
      "source": [
        "## Cách 1: Finetune\n",
        "\n",
        "Ta sẽ thử fine-tune model với pretrained có sẵn, các tham số theo như mặc định (tham khảo file yolov3/train.py), đây là cách chúng ta luôn thử đầu tiên với một bài toán mới. Bạn có thể thay đổi số epoch để rút ngắn thời gian training. Sau khi kết thúc quá trình training, hãy nhìn vào phần log cuối cùng để xác định vị trí lưu weights đã train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ok3gZu7W0f2g",
        "outputId": "2d6f8a9d-838e-4965-8a5e-b9410868cb01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: (30 second timeout) \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B disabled due to login timeout.\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov3.pt, cfg=, data=data/coconut.yaml, hyp=data/hyps/hyp.scratch.yaml, epochs=20, batch_size=16, imgsz=416, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, adam=False, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, patience=100, freeze=0, save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov3 ✅\n",
            "YOLOv3 🚀 v9.6.0-22-g0bbd055 torch 1.12.1+cu113 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv3 🚀 runs (RECOMMENDED)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Downloading https://github.com/ultralytics/yolov3/releases/download/v9.6.0/yolov3.pt to yolov3.pt...\n",
            "100% 119M/119M [00:03<00:00, 38.7MB/s]\n",
            "\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1       928  models.common.Conv                      [3, 32, 3, 1]                 \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     20672  models.common.Bottleneck                [64, 64]                      \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    164608  models.common.Bottleneck                [128, 128]                    \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  8   2627584  models.common.Bottleneck                [256, 256]                    \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  8  10498048  models.common.Bottleneck                [512, 512]                    \n",
            "  9                -1  1   4720640  models.common.Conv                      [512, 1024, 3, 2]             \n",
            " 10                -1  4  20983808  models.common.Bottleneck                [1024, 1024]                  \n",
            " 11                -1  1   5245952  models.common.Bottleneck                [1024, 1024, False]           \n",
            " 12                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
            " 13                -1  1   4720640  models.common.Conv                      [512, 1024, 3, 1]             \n",
            " 14                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
            " 15                -1  1   4720640  models.common.Conv                      [512, 1024, 3, 1]             \n",
            " 16                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 17                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 18           [-1, 8]  1         0  models.common.Concat                    [1]                           \n",
            " 19                -1  1   1377792  models.common.Bottleneck                [768, 512, False]             \n",
            " 20                -1  1   1312256  models.common.Bottleneck                [512, 512, False]             \n",
            " 21                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 22                -1  1   1180672  models.common.Conv                      [256, 512, 3, 1]              \n",
            " 23                -2  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 24                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 25           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 26                -1  1    344832  models.common.Bottleneck                [384, 256, False]             \n",
            " 27                -1  2    656896  models.common.Bottleneck                [256, 256, False]             \n",
            " 28      [27, 22, 15]  1     32310  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [256, 512, 1024]]\n",
            "Model Summary: 333 layers, 61523734 parameters, 61523734 gradients, 155.3 GFLOPs\n",
            "\n",
            "Transferred 433/439 items from yolov3.pt\n",
            "Scaled weight_decay = 0.0005\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 72 weight, 75 weight (no decay), 75 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(always_apply=False, p=0.01, blur_limit=(3, 7)), MedianBlur(always_apply=False, p=0.01, blur_limit=(3, 7)), ToGray(always_apply=False, p=0.01), CLAHE(always_apply=False, p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '../datasets/labels' images and labels...20 found, 0 missing, 0 empty, 0 corrupted: 100% 20/20 [00:00<00:00, 324.80it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: ../datasets/labels.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '../datasets/labels.cache' images and labels... 20 found, 0 missing, 0 empty, 0 corrupted: 100% 20/20 [00:00<?, ?it/s]\n",
            "Plotting labels to runs/train/exp/labels.jpg... \n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m6.04 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n",
            "Image sizes 416 train, 416 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp\u001b[0m\n",
            "Starting training for 20 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      0/19     5.55G    0.1223    0.0619         0        61       416: 100% 2/2 [00:09<00:00,  4.91s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:01<00:00,  1.44s/it]\n",
            "                 all         20        104     0.0434      0.106     0.0165    0.00305\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      1/19     5.55G    0.1187   0.04432         0        28       416: 100% 2/2 [00:00<00:00,  2.47it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  2.48it/s]\n",
            "                 all         20        104     0.0448     0.0865     0.0163    0.00306\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      2/19     5.55G    0.1194   0.05066         0        34       416: 100% 2/2 [00:00<00:00,  2.58it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  3.16it/s]\n",
            "                 all         20        104     0.0511     0.0962     0.0184    0.00313\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      3/19     5.55G    0.1189   0.04794         0        34       416: 100% 2/2 [00:00<00:00,  2.98it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  2.77it/s]\n",
            "                 all         20        104      0.051      0.115     0.0186    0.00346\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      4/19     5.55G    0.1174   0.05679         0        48       416: 100% 2/2 [00:00<00:00,  3.08it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  2.54it/s]\n",
            "                 all         20        104     0.0535      0.115     0.0226    0.00422\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      5/19     5.55G     0.117    0.0559         0        41       416: 100% 2/2 [00:00<00:00,  2.71it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  3.31it/s]\n",
            "                 all         20        104     0.0565      0.106     0.0231    0.00463\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      6/19     5.55G    0.1155    0.0653         0        42       416: 100% 2/2 [00:00<00:00,  2.69it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  3.08it/s]\n",
            "                 all         20        104       0.06      0.115     0.0262    0.00509\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      7/19     5.55G    0.1123   0.06001         0        34       416: 100% 2/2 [00:00<00:00,  2.72it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  2.75it/s]\n",
            "                 all         20        104     0.0644      0.125     0.0304    0.00557\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      8/19     5.55G    0.1084   0.06294         0        34       416: 100% 2/2 [00:00<00:00,  2.73it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  2.99it/s]\n",
            "                 all         20        104      0.218     0.0673     0.0401    0.00673\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      9/19     5.55G    0.1085   0.06882         0        46       416: 100% 2/2 [00:00<00:00,  2.74it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  3.09it/s]\n",
            "                 all         20        104     0.0817      0.135     0.0419    0.00709\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     10/19     5.55G    0.1085   0.05616         0        37       416: 100% 2/2 [00:00<00:00,  2.76it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  2.78it/s]\n",
            "                 all         20        104      0.102      0.144     0.0524    0.00852\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     11/19     5.55G    0.1067   0.06275         0        33       416: 100% 2/2 [00:00<00:00,  2.98it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  2.87it/s]\n",
            "                 all         20        104      0.098      0.183     0.0548     0.0103\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     12/19     5.55G    0.1077   0.07933         0        48       416: 100% 2/2 [00:00<00:00,  2.79it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  3.62it/s]\n",
            "                 all         20        104      0.099      0.183     0.0586     0.0107\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     13/19     5.55G    0.1034   0.05839         0        22       416: 100% 2/2 [00:00<00:00,  3.29it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  2.88it/s]\n",
            "                 all         20        104      0.227      0.115      0.072     0.0135\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     14/19     5.55G    0.1026   0.07506         0        41       416: 100% 2/2 [00:00<00:00,  2.82it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  2.90it/s]\n",
            "                 all         20        104      0.219      0.125     0.0819     0.0164\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     15/19     5.55G    0.1016   0.07169         0        34       416: 100% 2/2 [00:00<00:00,  2.57it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  2.76it/s]\n",
            "                 all         20        104      0.342      0.115     0.0881     0.0153\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     16/19     5.55G    0.1007   0.06679         0        29       416: 100% 2/2 [00:00<00:00,  2.61it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  2.86it/s]\n",
            "                 all         20        104      0.282      0.125     0.0979     0.0194\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     17/19     5.55G     0.104   0.08023         0        53       416: 100% 2/2 [00:00<00:00,  2.82it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  2.39it/s]\n",
            "                 all         20        104      0.144      0.221      0.107     0.0214\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     18/19     5.55G   0.09873   0.07616         0        39       416: 100% 2/2 [00:00<00:00,  3.49it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  3.56it/s]\n",
            "                 all         20        104      0.218      0.192      0.112     0.0215\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     19/19     5.55G    0.0979   0.07868         0        38       416: 100% 2/2 [00:00<00:00,  2.66it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  2.86it/s]\n",
            "                 all         20        104      0.243      0.192      0.115     0.0209\n",
            "\n",
            "20 epochs completed in 0.032 hours.\n",
            "Optimizer stripped from runs/train/exp/weights/last.pt, 123.4MB\n",
            "Optimizer stripped from runs/train/exp/weights/best.pt, 123.4MB\n",
            "\n",
            "Validating runs/train/exp/weights/best.pt...\n",
            "Fusing layers... \n",
            "Model Summary: 261 layers, 61497430 parameters, 0 gradients, 154.5 GFLOPs\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  1.74it/s]\n",
            "                 all         20        104       0.22      0.192      0.112     0.0216\n",
            "Results saved to \u001b[1mruns/train/exp\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python train.py --img 416 --batch 16 --epochs 20 --data data/coconut.yaml --weights yolov3.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mG8oitE0n2LY"
      },
      "source": [
        "Thử inference trên 1 video sử dụng model đã finetune ở trên, ta sẽ sử dụng IoU threshold cho phần NMS là 0.5, threshold cho object score là 0.6  \n",
        "\n",
        "Tải video tại link sau: ***https://u.pcloud.link/publink/show?code=XZziBhVZgRqC3dfFdWL1kB1zq0py6BbrMJny***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rvmwahle_vg-",
        "outputId": "01e847eb-d273-4d38-dca2-103c3688a350"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['runs/train/exp/weights/best.pt'], source=../datasets/coconut.mp4, imgsz=[416, 416], conf_thres=0.6, iou_thres=0.5, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
            "YOLOv3 🚀 v9.6.0-3-gb870de5 torch 1.10.0+cu111 CUDA:0 (Tesla K80, 11441MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 261 layers, 61497430 parameters, 0 gradients, 154.7 GFLOPs\n",
            "Traceback (most recent call last):\n",
            "  File \"detect.py\", line 244, in <module>\n",
            "    main(opt)\n",
            "  File \"detect.py\", line 239, in main\n",
            "    run(**vars(opt))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\", line 28, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"detect.py\", line 95, in run\n",
            "    dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt and not jit)\n",
            "  File \"/content/yolov3/utils/datasets.py\", line 170, in __init__\n",
            "    raise Exception(f'ERROR: {p} does not exist')\n",
            "Exception: ERROR: /content/datasets/coconut.mp4 does not exist\n"
          ]
        }
      ],
      "source": [
        "!python detect.py --weights runs/train/exp/weights/best.pt --source ../datasets/coconut.mp4 --imgsz 416 --iou-thres 0.5 --conf-thres 0.6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nRsKhiTofTY"
      },
      "source": [
        "## Cách 2: Tự cấu hình lại các tham số\n",
        "\n",
        "Để làm chủ việc training model, ta có thể tự thay đổi các tham số để phù hợp với từng bộ dữ liệu khác nhau. Bài toán coconut detection khá đơn giản, vì vậy cách sử dụng pretrained ở trên đã cho kết quả tốt, tuy nhiên với nhiều bài toán phức tạp hơn, ta sẽ cần thay đổi các tham số sao cho phù hợp. Ở đây ta sẽ cấu hình 2 phần chính:\n",
        "\n",
        "\n",
        "*   Anchor boxes và Model architecture\n",
        "*   Model hyper parameters\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rr1anWZexdEE"
      },
      "source": [
        "### Anchor boxes và Model architecture\n",
        "\n",
        "- Hãy quan sát file yolov3/models/yolov3.yaml. Ta sẽ tạo file yolov3/models/coconut.yaml để tự cấu hình.\n",
        "- Phần architecture gồm backbone và head bạn có thể tự thay đổi ở nhà để thử nghiệm, trong bài thực hành này ta sẽ giữ nguyên architecture của yolov3\n",
        "- Tham số _nc_ là số lượng lớp của bài toán, ta cần thay đổi cho phù hợp (ở cách train 1 ở trên, ta không chỉ định file config yaml này thì model tự hiểu lấy tham số _nc_ trong file data/coconut.yaml)\n",
        "- Tham số anchors: Trong nhiều bài toán phức tạp, việc tìm anchor boxes mới là cần thiết. Bạn có thể tham khảo repo sau: <a href=https://github.com/decanbay/YOLOv3-Calculate-Anchor-Boxes>https://github.com/decanbay/YOLOv3-Calculate-Anchor-Boxes </a>. Hãy thử thay đổi anchor box của mô hình như sau:\n",
        "\n",
        "```\n",
        "P3/8: 77,34, 84,50, 87,70\n",
        "\n",
        "P4/16: 100,43, 107,59, 117,75\n",
        "\n",
        "P5/32: 128,50, 142,66, 153,87\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tztgjl7Lz7Sj"
      },
      "source": [
        "### Model hyper parameter\n",
        "\n",
        "Các siêu tham số cho quá trình huấn luyện mô hình được lưu ở các file yaml trong thư mục yolov3/data/hyps. Hãy quan sát file hyp.scratch.yaml và thay đổi theo ý muốn của bạn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAkEBi6V0Zm7"
      },
      "source": [
        "### Training model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAnNcXEmvnCq"
      },
      "outputs": [],
      "source": [
        "!python train.py --img 416 --batch 16 --epochs 20 --data data/coconut.yaml --weights yolov3.pt --cfg models/coconut.yaml --hyp data/hyps/hyp.scratch.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4cV_TtM02uL"
      },
      "source": [
        "# 5. Làm chủ pipeline inference model đã train\n",
        "\n",
        "Ở phần trên chúng ta sử dụng file detect.py để infer mô hình đã huấn luyện, ở phần này chúng ta sẽ dựa vào file detect.py, viết 1 hàm detector ngắn gọn, nhận vào 1 ảnh, trả lại tọa độ các bounding box của object. Việc viết hàm này sẽ giúp phần lập trình thuật toán tracking dễ dàng hơn trong các buổi sau"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IjVeIWm1lmm",
        "outputId": "af84b978-4f6b-414f-db88-855851f424c5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "YOLOv3 🚀 v9.6.0-3-gb870de5 torch 1.10.0+cu111 CPU\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 261 layers, 61497430 parameters, 0 gradients, 154.7 GFLOPs\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DetectMultiBackend() 32\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import cv2\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import numpy as np\n",
        "\n",
        "from models.common import DetectMultiBackend\n",
        "from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)\n",
        "from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox\n",
        "\n",
        "class YOLO():\n",
        "  def __init__(self):\n",
        "    self.img_size = [416, 416]\n",
        "    self.model_path = 'runs/train/exp/weights/best.pt'\n",
        "    self.iou_threshold = 0.5\n",
        "    self.conf_threshold = 0.6\n",
        "    self.device = 'cpu'\n",
        "    self.device = select_device(self.device)\n",
        "\n",
        "    # Initial model\n",
        "    self.model = self.load_models()\n",
        "    self.stride = self.model.stride\n",
        "    self.classes = self.model.names\n",
        "    print(self.model, self.stride)\n",
        "\n",
        "  def load_models(self):\n",
        "    model = DetectMultiBackend(self.model_path, device=self.device)\n",
        "    model.model.float()\n",
        "    # Warmup\n",
        "    model(torch.zeros(1, 3, *self.img_size).to(self.device).type_as(next(model.model.parameters())))\n",
        "    return model\n",
        "\n",
        "  def preprocess_image(self, image):\n",
        "    image = letterbox(image, self.img_size, stride=self.stride)[0]\n",
        "    image = image.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    image = np.ascontiguousarray(image)\n",
        "\n",
        "    return image\n",
        "\n",
        "  def detect(self, image):\n",
        "    im = np.array([self.preprocess_image(image)])\n",
        "    im = torch.from_numpy(im).to(self.device)\n",
        "    im = im.float() / 255.0\n",
        "    pred = self.model(im)\n",
        "    pred = non_max_suppression(pred, self.conf_threshold, self.iou_threshold)\n",
        "\n",
        "    original_image = image.copy()\n",
        "    centers = []\n",
        "    bboxes = []\n",
        "    obj_type = []\n",
        "\n",
        "    for i, det in enumerate(pred):\n",
        "      det[:, :4] = scale_coords(im.shape[2:], det[:, :4], image.shape).round()\n",
        "\n",
        "      for *xyxy, conf, cls in reversed(det):\n",
        "        xyxy = [float(a) for a in xyxy]\n",
        "        bboxes.append(np.array(xyxy))\n",
        "\n",
        "        x_center = (xyxy[0] + xyxy[2]) / 2\n",
        "        y_center = (xyxy[1] + xyxy[3]) / 2\n",
        "        centroid = np.array([[x_center], [y_center]])\n",
        "        centers.append(np.round(centroid))\n",
        "\n",
        "        predicted_class = self.classes[int(cls)]\n",
        "        obj_type.append(predicted_class)\n",
        "\n",
        "    return original_image, centers, bboxes, obj_type\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  image = cv2.imread('../datasets/images/5.png')\n",
        "  #############\n",
        "# Khởi tạo YOLO\n",
        "#############\n",
        "  original_image, centers, bboxes, obj_type = detector.detect(image)\n",
        "  print(centers)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
